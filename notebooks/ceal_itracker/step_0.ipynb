{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0db37f",
   "metadata": {},
   "source": [
    "# Step -1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63248852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system/file stuff\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# datasci stuff\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "# computer vision stuff\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# ML stuff\n",
    "import torch\n",
    "from tqdm import tqdm # bc i need progress bars\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# detect face/eye regions of image\n",
    "import mediapipe as mp\n",
    "\n",
    "# augmentation\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516da167",
   "metadata": {},
   "source": [
    "# Step 0: Establish Dummy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be78fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITrackerModel.py exists? True\n",
      "checkpoint exists? True\n"
     ]
    }
   ],
   "source": [
    "itracker_path = Path(\"/Volumes/Crucial X10/210/GazeCapture/pytorch\")\n",
    "sys.path.append(str(itracker_path)) \n",
    "\n",
    "# sanity\n",
    "print(\"ITrackerModel.py exists?\", (itracker_path / \"ITrackerModel.py\").exists())\n",
    "print(\"checkpoint exists?\", (itracker_path / \"checkpoint.pth.tar\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09ac82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an empty iTracker network (random weights).”\n",
    "from ITrackerModel import ITrackerModel\n",
    "device = \"cpu\"\n",
    "model = ITrackerModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46512875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained weights into model\n",
    "ckpt = torch.load(itracker_path / \"checkpoint.pth.tar\", map_location=device)\n",
    "state = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5803a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: []\n",
      "unexpected: []\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(\"missing:\", missing)\n",
    "print(\"unexpected:\", unexpected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09aed84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape: torch.Size([1, 2])\n",
      "out: tensor([[-0.1647, -9.3985]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "B = 1\n",
    "faces = torch.randn(B, 3, 224, 224, device=device)\n",
    "eyesL = torch.randn(B, 3, 224, 224, device=device)\n",
    "eyesR = torch.randn(B, 3, 224, 224, device=device)\n",
    "faceGrid = torch.randn(B, 25, 25, device=device)  # matches gridSize=25\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(faces, eyesL, eyesR, faceGrid)\n",
    "\n",
    "print(\"out shape:\", out.shape)\n",
    "print(\"out:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fe81b",
   "metadata": {},
   "source": [
    "# Step 1: Confirm Expected Inputs\n",
    "\n",
    "Confirm expected inputs: face/left/right tensor shape, dtype, normalization steps (mean images).\n",
    "\n",
    "This information was used to help create eye2voice/code/ceal_itracker/data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "117c649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face: (224, 224, 3) float32 79.87755584716797 180.56549072265625\n",
      "left: (224, 224, 3) float32 72.00033569335938 191.53338623046875\n",
      "right: (224, 224, 3) float32 71.83743286132812 177.91868591308594\n"
     ]
    }
   ],
   "source": [
    "mean_face = scipy.io.loadmat(itracker_path / \"mean_face_224.mat\")[\"image_mean\"]\n",
    "mean_left = scipy.io.loadmat(itracker_path / \"mean_left_224.mat\")[\"image_mean\"]\n",
    "mean_right = scipy.io.loadmat(itracker_path / \"mean_right_224.mat\")[\"image_mean\"]\n",
    "\n",
    "print(\"face:\", mean_face.shape, mean_face.dtype, float(mean_face.min()), float(mean_face.max()))\n",
    "print(\"left:\", mean_left.shape, mean_left.dtype, float(mean_left.min()), float(mean_left.max()))\n",
    "print(\"right:\", mean_right.shape, mean_right.dtype, float(mean_right.min()), float(mean_right.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9cf3b",
   "metadata": {},
   "source": [
    "# Step 2: Augmentation + Cropping Pipeline\n",
    "\n",
    "produce face/left/right 224×224 crops for CEAL (using CEAL metadata or a detector).\n",
    "\n",
    "for now, to get the pipeline up and running, we will use CEAL metadata to determine how to crop images. this works because the original images are so consistent with the camera always 2m away and the subjects positioned uniformly across images. \n",
    "\n",
    "we will need to implement a detector model thing later to automatically detect face/eye regions in order to perform the cropping.\n",
    "\n",
    "jk implementing that now bc the other way got too complex and i knew we would need this anyway\n",
    "\n",
    "this is required to match how iTracker model ingests information and produces output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa126",
   "metadata": {},
   "source": [
    "## Step 2.0: Metadata\n",
    "\n",
    "File name parsing and label creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d394261e",
   "metadata": {},
   "source": [
    "## Step 2.1: Landmark Detection\n",
    "\n",
    "Use MediaPipe Face Mesh (fast, easy to install, no compiling dlib misery).\n",
    "\n",
    "Output: face bbox + left-eye region bbox + right-eye region bbox (or keypoints you convert to bboxes)\n",
    "\n",
    "Limitation: the repo’s original crops come from Apple’s detector JSONs. Your MediaPipe boxes will be different. That’s okay, but don’t expect perfect pretrained performance unless your crop style is similar. (our crop style is probably going to be similar, i hope?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eabd51",
   "metadata": {},
   "source": [
    "## Step 2.2: Augment full image with keypoints\n",
    "\n",
    "We need augmentation to be consistent across our 3 crops (face, left eye, right eye)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d77bb",
   "metadata": {},
   "source": [
    "## Step 2.3: Crop & Resize\n",
    "\n",
    "Crop face, left_eye, right_eye from augmented full image\n",
    "\n",
    "Resize each to (224,224)\n",
    "\n",
    "create correct manifest csv to ensure the crops that belong to the same face stay together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60383c1a",
   "metadata": {},
   "source": [
    "## Step 2.4: Facegrid\n",
    "\n",
    "Create a 25×25 binary array, set to 1s in the region corresponding to the face bbox projected into that grid.\n",
    "\n",
    "Yes this is another iTracker requirement\n",
    "\n",
    "uhg the original preprocessing pipeline did not use pixels but instead used grid coordinates. so new plan:\n",
    "\n",
    "Compute (grid_x, grid_y, grid_w, grid_h) in 25×25 units from the face bbox in full-frame coords.\n",
    "\n",
    "Then precompute and store the 625-length flattened grid. (where is this used later? in data.py?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945bf6b",
   "metadata": {},
   "source": [
    "## Step 2.5: Save Crops and Facegrid\n",
    "\n",
    "make sure the tensors you return match:\n",
    "\n",
    "return row, imFace, imEyeL, imEyeR, faceGrid, label\n",
    "\n",
    "save in OUTPUT_ROOT = Path(\"/Volumes/Crucial X10/210/data/ceal_augmented_cropped\")\n",
    "\n",
    "ensure data.py matches\n",
    "\n",
    "ensure the crops that belong to the same face stay together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae8907",
   "metadata": {},
   "source": [
    "## Step 2.6: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest image\n",
    "# crop face\n",
    "# crop eyes\n",
    "# augment 3 crops together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(val, lo, hi):\n",
    "    return max(lo, min(hi, val))\n",
    "\n",
    "def crop_with_padding(img: Image.Image, box, pad_frac=0.08) -> Image.Image:\n",
    "    \"\"\"\n",
    "    img: PIL.Image RGB\n",
    "    box: (x1, y1, x2, y2) in pixel coords\n",
    "    pad_frac: fraction of box size to pad on each side\n",
    "    \"\"\"\n",
    "    W, H = img.size\n",
    "    x1, y1, x2, y2 = box\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "\n",
    "    pad_x = int(w * pad_frac)\n",
    "    pad_y = int(h * pad_frac)\n",
    "\n",
    "    x1p = clamp(x1 - pad_x, 0, W)\n",
    "    y1p = clamp(y1 - pad_y, 0, H)\n",
    "    x2p = clamp(x2 + pad_x, 0, W)\n",
    "    y2p = clamp(y2 + pad_y, 0, H)\n",
    "\n",
    "    return img.crop((x1p, y1p, x2p, y2p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef255d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_eye_boxes_from_face(face_box):\n",
    "    \"\"\"\n",
    "    face_box: (x1, y1, x2, y2) in original image coords\n",
    "    returns: (left_eye_box, right_eye_box) as pixel boxes\n",
    "\n",
    "    These ratios are “reasonable defaults” for a forward-facing face:\n",
    "        Eyes live in the upper half of the face crop\n",
    "        Left eye is in the left half; right eye in the right half\n",
    "        We’ll take a rectangle that is ~40% of face width and ~25% of face height\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = face_box\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "\n",
    "    # region where eyes likely live (upper portion)\n",
    "    eye_region_top = y1 + int(0.18 * h)\n",
    "    eye_region_bot = y1 + int(0.55 * h)\n",
    "\n",
    "    # each eye box width/height\n",
    "    eye_w = int(0.40 * w)\n",
    "    eye_h = eye_region_bot - eye_region_top\n",
    "\n",
    "    # centers for left/right eyes\n",
    "    left_cx  = x1 + int(0.33 * w)\n",
    "    right_cx = x1 + int(0.67 * w)\n",
    "\n",
    "    def box_from_center(cx):\n",
    "        ex1 = cx - eye_w // 2\n",
    "        ex2 = cx + eye_w // 2\n",
    "        ey1 = eye_region_top\n",
    "        ey2 = eye_region_bot\n",
    "        return (ex1, ey1, ex2, ey2)\n",
    "\n",
    "    return box_from_center(left_cx), box_from_center(right_cx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceal_crops_from_face_box(img_rgb: Image.Image, face_box):\n",
    "    \"\"\"\n",
    "    img_rgb: PIL.Image in RGB\n",
    "    face_box: (x1, y1, x2, y2) in original image coordinates\n",
    "    returns: face_pil, left_pil, right_pil\n",
    "    \"\"\"\n",
    "    # Face crop (pad a bit)\n",
    "    face_pil = crop_with_padding(img_rgb, face_box, pad_frac=0.06)\n",
    "\n",
    "    # Eye boxes derived in original coordinates, then cropped from original image\n",
    "    left_box, right_box = derive_eye_boxes_from_face(face_box)\n",
    "\n",
    "    left_pil  = crop_with_padding(img_rgb, left_box, pad_frac=0.10)\n",
    "    right_pil = crop_with_padding(img_rgb, right_box, pad_frac=0.10)\n",
    "\n",
    "    return face_pil, left_pil, right_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74df3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_crops(img_rgb):\n",
    "    W, H = img_rgb.size\n",
    "\n",
    "    # Face region (ratios tuned for centered head + chin rest)\n",
    "    face_box = (int(0.30*W), int(0.12*H), int(0.70*W), int(0.92*H))\n",
    "\n",
    "    # Eye band inside the face region\n",
    "    ex1, ey1, ex2, ey2 = (int(0.33*W), int(0.30*H), int(0.67*W), int(0.55*H))\n",
    "    eye_band = img_rgb.crop((ex1, ey1, ex2, ey2))\n",
    "    bw, bh = eye_band.size\n",
    "\n",
    "    left_eye  = eye_band.crop((0, 0, bw//2, bh))\n",
    "    right_eye = eye_band.crop((bw//2, 0, bw, bh))\n",
    "\n",
    "    face = img_rgb.crop(face_box)\n",
    "\n",
    "    return face, left_eye, right_eye\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd96369",
   "metadata": {},
   "source": [
    "# Step 3: Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rgb_pil(pil_img: Image.Image, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    pil_img.save(path, format=\"JPEG\", quality=95)\n",
    "\n",
    "def build_itracker_crops_manifest(df, output_root: Path, num_augmentations: int):\n",
    "    \"\"\"\n",
    "    df columns expected (adapt as needed):\n",
    "      - path (original image path)\n",
    "      - subject\n",
    "      - filename or stem\n",
    "      - label\n",
    "    \"\"\"\n",
    "    output_root = Path(output_root)\n",
    "    rows = []\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        img_bgr = cv2.imread(r[\"path\"])\n",
    "        if img_bgr is None:\n",
    "            continue\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil = Image.fromarray(img_rgb).convert(\"RGB\")\n",
    "\n",
    "        stem = Path(r.get(\"filename\", Path(r[\"path\"]).name)).stem\n",
    "        subj = str(r[\"subject\"])\n",
    "\n",
    "        # ---- 1) Crop first (YOU plug in your logic) ----\n",
    "       \n",
    "        # determine bos to use for cropping around face\n",
    "        W, H = pil.size\n",
    "        face_box = (\n",
    "            int(0.15 * W),\n",
    "            int(0.12 * H),\n",
    "            int(0.85 * W),\n",
    "            int(0.92 * H),\n",
    ")\n",
    "        face_pil, left_pil, right_pil = ceal_crops_from_face_box(pil)\n",
    "\n",
    "        # ---- 2) Save original crops (aug_id=0) ----\n",
    "        base_dir = output_root / subj / stem\n",
    "        face0 = base_dir / \"face_000.jpg\"\n",
    "        left0 = base_dir / \"left_000.jpg\"\n",
    "        right0 = base_dir / \"right_000.jpg\"\n",
    "\n",
    "        save_rgb_pil(face_pil.resize((224, 224)), face0)\n",
    "        save_rgb_pil(left_pil.resize((224, 224)), left0)\n",
    "        save_rgb_pil(right_pil.resize((224, 224)), right0)\n",
    "\n",
    "        rows.append({\n",
    "            **r.to_dict(),\n",
    "            \"aug_id\": 0,\n",
    "            \"is_augmented\": False,\n",
    "            \"face_path\": str(face0),\n",
    "            \"left_path\": str(left0),\n",
    "            \"right_path\": str(right0),\n",
    "        })\n",
    "\n",
    "        # ---- 3) Augment the CROPS (keep in sync) ----\n",
    "        for k in range(1, num_augmentations + 1):\n",
    "            face_aug, left_aug, right_aug, label_aug = augment_triplet(\n",
    "                face_pil, left_pil, right_pil, r[\"label\"]\n",
    "            )\n",
    "\n",
    "            facek = base_dir / f\"face_{k:03d}.jpg\"\n",
    "            leftk = base_dir / f\"left_{k:03d}.jpg\"\n",
    "            rightk = base_dir / f\"right_{k:03d}.jpg\"\n",
    "\n",
    "            save_rgb_pil(face_aug.resize((224, 224)), facek)\n",
    "            save_rgb_pil(left_aug.resize((224, 224)), leftk)\n",
    "            save_rgb_pil(right_aug.resize((224, 224)), rightk)\n",
    "\n",
    "            rows.append({\n",
    "                **r.to_dict(),\n",
    "                \"label\": label_aug,\n",
    "                \"aug_id\": k,\n",
    "                \"is_augmented\": True,\n",
    "                \"face_path\": str(facek),\n",
    "                \"left_path\": str(leftk),\n",
    "                \"right_path\": str(rightk),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a09d5",
   "metadata": {},
   "source": [
    "### Debug start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "266456fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN_PATH: .\n",
      "exists: True\n",
      "contents sample: ['step_0.ipynb']\n",
      "mean_face exists: False\n",
      "mean_face path: mean_face_224.mat\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ITrackerData import MEAN_PATH\n",
    "\n",
    "mean_dir = Path(MEAN_PATH)\n",
    "print(\"MEAN_PATH:\", mean_dir)\n",
    "print(\"exists:\", mean_dir.exists())\n",
    "print(\"contents sample:\", [p.name for p in mean_dir.glob(\"*\")][:20])\n",
    "\n",
    "mean_file = mean_dir / \"mean_face_224.mat\"\n",
    "print(\"mean_face exists:\", mean_file.exists())\n",
    "print(\"mean_face path:\", mean_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037c077",
   "metadata": {},
   "source": [
    "### debug end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c084443",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
